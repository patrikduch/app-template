// Code generated by SQLBoiler 4.13.0 (https://github.com/volatiletech/sqlboiler). DO NOT EDIT.
// This file is meant to be re-generated in place and/or deleted at any time.

package models

import (
	"context"
	"database/sql"
	"fmt"
	"reflect"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/friendsofgo/errors"
	"github.com/volatiletech/null/v8"
	"github.com/volatiletech/sqlboiler/v4/boil"
	"github.com/volatiletech/sqlboiler/v4/queries"
	"github.com/volatiletech/sqlboiler/v4/queries/qm"
	"github.com/volatiletech/sqlboiler/v4/queries/qmhelper"
	"github.com/volatiletech/sqlboiler/v4/types"
	"github.com/volatiletech/strmangle"
)

// AttributeSchema is an object representing the database table.
type AttributeSchema struct {
	ID             int               `boil:"id" json:"id" toml:"id" yaml:"id"`
	AssetType      string            `boil:"asset_type" json:"asset_type" toml:"asset_type" yaml:"asset_type"`
	AttributeType  null.String       `boil:"attribute_type" json:"attribute_type,omitempty" toml:"attribute_type" yaml:"attribute_type,omitempty"`
	Attribute      string            `boil:"attribute" json:"attribute" toml:"attribute" yaml:"attribute"`
	Subtype        string            `boil:"subtype" json:"subtype" toml:"subtype" yaml:"subtype"`
	Enable         bool              `boil:"enable" json:"enable" toml:"enable" yaml:"enable"`
	Translation    null.JSON         `boil:"translation" json:"translation,omitempty" toml:"translation" yaml:"translation,omitempty"`
	Unit           null.String       `boil:"unit" json:"unit,omitempty" toml:"unit" yaml:"unit,omitempty"`
	Formula        null.String       `boil:"formula" json:"formula,omitempty" toml:"formula" yaml:"formula,omitempty"`
	Scale          types.NullDecimal `boil:"scale" json:"scale,omitempty" toml:"scale" yaml:"scale,omitempty"`
	Zero           null.Float64      `boil:"zero" json:"zero,omitempty" toml:"zero" yaml:"zero,omitempty"`
	Precision      null.Int16        `boil:"precision" json:"precision,omitempty" toml:"precision" yaml:"precision,omitempty"`
	Min            types.NullDecimal `boil:"min" json:"min,omitempty" toml:"min" yaml:"min,omitempty"`
	IsDigital      bool              `boil:"is_digital" json:"is_digital" toml:"is_digital" yaml:"is_digital"`
	Max            types.NullDecimal `boil:"max" json:"max,omitempty" toml:"max" yaml:"max,omitempty"`
	Step           types.NullDecimal `boil:"step" json:"step,omitempty" toml:"step" yaml:"step,omitempty"`
	Map            null.JSON         `boil:"map" json:"map,omitempty" toml:"map" yaml:"map,omitempty"`
	PipelineMode   null.String       `boil:"pipeline_mode" json:"pipeline_mode,omitempty" toml:"pipeline_mode" yaml:"pipeline_mode,omitempty"`
	PipelineRaster types.StringArray `boil:"pipeline_raster" json:"pipeline_raster,omitempty" toml:"pipeline_raster" yaml:"pipeline_raster,omitempty"`
	Viewer         bool              `boil:"viewer" json:"viewer" toml:"viewer" yaml:"viewer"`
	Ar             bool              `boil:"ar" json:"ar" toml:"ar" yaml:"ar"`
	Seq            null.Int16        `boil:"seq" json:"seq,omitempty" toml:"seq" yaml:"seq,omitempty"`
	SourcePath     types.StringArray `boil:"source_path" json:"source_path,omitempty" toml:"source_path" yaml:"source_path,omitempty"`
	Virtual        null.Bool         `boil:"virtual" json:"virtual,omitempty" toml:"virtual" yaml:"virtual,omitempty"`

	R *attributeSchemaR `boil:"-" json:"-" toml:"-" yaml:"-"`
	L attributeSchemaL  `boil:"-" json:"-" toml:"-" yaml:"-"`
}

var AttributeSchemaColumns = struct {
	ID             string
	AssetType      string
	AttributeType  string
	Attribute      string
	Subtype        string
	Enable         string
	Translation    string
	Unit           string
	Formula        string
	Scale          string
	Zero           string
	Precision      string
	Min            string
	IsDigital      string
	Max            string
	Step           string
	Map            string
	PipelineMode   string
	PipelineRaster string
	Viewer         string
	Ar             string
	Seq            string
	SourcePath     string
	Virtual        string
}{
	ID:             "id",
	AssetType:      "asset_type",
	AttributeType:  "attribute_type",
	Attribute:      "attribute",
	Subtype:        "subtype",
	Enable:         "enable",
	Translation:    "translation",
	Unit:           "unit",
	Formula:        "formula",
	Scale:          "scale",
	Zero:           "zero",
	Precision:      "precision",
	Min:            "min",
	IsDigital:      "is_digital",
	Max:            "max",
	Step:           "step",
	Map:            "map",
	PipelineMode:   "pipeline_mode",
	PipelineRaster: "pipeline_raster",
	Viewer:         "viewer",
	Ar:             "ar",
	Seq:            "seq",
	SourcePath:     "source_path",
	Virtual:        "virtual",
}

var AttributeSchemaTableColumns = struct {
	ID             string
	AssetType      string
	AttributeType  string
	Attribute      string
	Subtype        string
	Enable         string
	Translation    string
	Unit           string
	Formula        string
	Scale          string
	Zero           string
	Precision      string
	Min            string
	IsDigital      string
	Max            string
	Step           string
	Map            string
	PipelineMode   string
	PipelineRaster string
	Viewer         string
	Ar             string
	Seq            string
	SourcePath     string
	Virtual        string
}{
	ID:             "attribute_schema.id",
	AssetType:      "attribute_schema.asset_type",
	AttributeType:  "attribute_schema.attribute_type",
	Attribute:      "attribute_schema.attribute",
	Subtype:        "attribute_schema.subtype",
	Enable:         "attribute_schema.enable",
	Translation:    "attribute_schema.translation",
	Unit:           "attribute_schema.unit",
	Formula:        "attribute_schema.formula",
	Scale:          "attribute_schema.scale",
	Zero:           "attribute_schema.zero",
	Precision:      "attribute_schema.precision",
	Min:            "attribute_schema.min",
	IsDigital:      "attribute_schema.is_digital",
	Max:            "attribute_schema.max",
	Step:           "attribute_schema.step",
	Map:            "attribute_schema.map",
	PipelineMode:   "attribute_schema.pipeline_mode",
	PipelineRaster: "attribute_schema.pipeline_raster",
	Viewer:         "attribute_schema.viewer",
	Ar:             "attribute_schema.ar",
	Seq:            "attribute_schema.seq",
	SourcePath:     "attribute_schema.source_path",
	Virtual:        "attribute_schema.virtual",
}

// Generated where

type whereHelpertypes_NullDecimal struct{ field string }

func (w whereHelpertypes_NullDecimal) EQ(x types.NullDecimal) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpertypes_NullDecimal) NEQ(x types.NullDecimal) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpertypes_NullDecimal) LT(x types.NullDecimal) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpertypes_NullDecimal) LTE(x types.NullDecimal) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpertypes_NullDecimal) GT(x types.NullDecimal) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpertypes_NullDecimal) GTE(x types.NullDecimal) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}

func (w whereHelpertypes_NullDecimal) IsNull() qm.QueryMod { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpertypes_NullDecimal) IsNotNull() qm.QueryMod {
	return qmhelper.WhereIsNotNull(w.field)
}

var AttributeSchemaWhere = struct {
	ID             whereHelperint
	AssetType      whereHelperstring
	AttributeType  whereHelpernull_String
	Attribute      whereHelperstring
	Subtype        whereHelperstring
	Enable         whereHelperbool
	Translation    whereHelpernull_JSON
	Unit           whereHelpernull_String
	Formula        whereHelpernull_String
	Scale          whereHelpertypes_NullDecimal
	Zero           whereHelpernull_Float64
	Precision      whereHelpernull_Int16
	Min            whereHelpertypes_NullDecimal
	IsDigital      whereHelperbool
	Max            whereHelpertypes_NullDecimal
	Step           whereHelpertypes_NullDecimal
	Map            whereHelpernull_JSON
	PipelineMode   whereHelpernull_String
	PipelineRaster whereHelpertypes_StringArray
	Viewer         whereHelperbool
	Ar             whereHelperbool
	Seq            whereHelpernull_Int16
	SourcePath     whereHelpertypes_StringArray
	Virtual        whereHelpernull_Bool
}{
	ID:             whereHelperint{field: "\"attribute_schema\".\"id\""},
	AssetType:      whereHelperstring{field: "\"attribute_schema\".\"asset_type\""},
	AttributeType:  whereHelpernull_String{field: "\"attribute_schema\".\"attribute_type\""},
	Attribute:      whereHelperstring{field: "\"attribute_schema\".\"attribute\""},
	Subtype:        whereHelperstring{field: "\"attribute_schema\".\"subtype\""},
	Enable:         whereHelperbool{field: "\"attribute_schema\".\"enable\""},
	Translation:    whereHelpernull_JSON{field: "\"attribute_schema\".\"translation\""},
	Unit:           whereHelpernull_String{field: "\"attribute_schema\".\"unit\""},
	Formula:        whereHelpernull_String{field: "\"attribute_schema\".\"formula\""},
	Scale:          whereHelpertypes_NullDecimal{field: "\"attribute_schema\".\"scale\""},
	Zero:           whereHelpernull_Float64{field: "\"attribute_schema\".\"zero\""},
	Precision:      whereHelpernull_Int16{field: "\"attribute_schema\".\"precision\""},
	Min:            whereHelpertypes_NullDecimal{field: "\"attribute_schema\".\"min\""},
	IsDigital:      whereHelperbool{field: "\"attribute_schema\".\"is_digital\""},
	Max:            whereHelpertypes_NullDecimal{field: "\"attribute_schema\".\"max\""},
	Step:           whereHelpertypes_NullDecimal{field: "\"attribute_schema\".\"step\""},
	Map:            whereHelpernull_JSON{field: "\"attribute_schema\".\"map\""},
	PipelineMode:   whereHelpernull_String{field: "\"attribute_schema\".\"pipeline_mode\""},
	PipelineRaster: whereHelpertypes_StringArray{field: "\"attribute_schema\".\"pipeline_raster\""},
	Viewer:         whereHelperbool{field: "\"attribute_schema\".\"viewer\""},
	Ar:             whereHelperbool{field: "\"attribute_schema\".\"ar\""},
	Seq:            whereHelpernull_Int16{field: "\"attribute_schema\".\"seq\""},
	SourcePath:     whereHelpertypes_StringArray{field: "\"attribute_schema\".\"source_path\""},
	Virtual:        whereHelpernull_Bool{field: "\"attribute_schema\".\"virtual\""},
}

// AttributeSchemaRels is where relationship names are stored.
var AttributeSchemaRels = struct {
}{}

// attributeSchemaR is where relationships are stored.
type attributeSchemaR struct {
}

// NewStruct creates a new relationship struct
func (*attributeSchemaR) NewStruct() *attributeSchemaR {
	return &attributeSchemaR{}
}

// attributeSchemaL is where Load methods for each relationship are stored.
type attributeSchemaL struct{}

var (
	attributeSchemaAllColumns            = []string{"id", "asset_type", "attribute_type", "attribute", "subtype", "enable", "translation", "unit", "formula", "scale", "zero", "precision", "min", "is_digital", "max", "step", "map", "pipeline_mode", "pipeline_raster", "viewer", "ar", "seq", "source_path", "virtual"}
	attributeSchemaColumnsWithoutDefault = []string{"asset_type", "attribute"}
	attributeSchemaColumnsWithDefault    = []string{"id", "attribute_type", "subtype", "enable", "translation", "unit", "formula", "scale", "zero", "precision", "min", "is_digital", "max", "step", "map", "pipeline_mode", "pipeline_raster", "viewer", "ar", "seq", "source_path", "virtual"}
	attributeSchemaPrimaryKeyColumns     = []string{"id"}
	attributeSchemaGeneratedColumns      = []string{}
)

type (
	// AttributeSchemaSlice is an alias for a slice of pointers to AttributeSchema.
	// This should almost always be used instead of []AttributeSchema.
	AttributeSchemaSlice []*AttributeSchema
	// AttributeSchemaHook is the signature for custom AttributeSchema hook methods
	AttributeSchemaHook func(context.Context, boil.ContextExecutor, *AttributeSchema) error

	attributeSchemaQuery struct {
		*queries.Query
	}
)

// Cache for insert, update and upsert
var (
	attributeSchemaType                 = reflect.TypeOf(&AttributeSchema{})
	attributeSchemaMapping              = queries.MakeStructMapping(attributeSchemaType)
	attributeSchemaPrimaryKeyMapping, _ = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, attributeSchemaPrimaryKeyColumns)
	attributeSchemaInsertCacheMut       sync.RWMutex
	attributeSchemaInsertCache          = make(map[string]insertCache)
	attributeSchemaUpdateCacheMut       sync.RWMutex
	attributeSchemaUpdateCache          = make(map[string]updateCache)
	attributeSchemaUpsertCacheMut       sync.RWMutex
	attributeSchemaUpsertCache          = make(map[string]insertCache)
)

var (
	// Force time package dependency for automated UpdatedAt/CreatedAt.
	_ = time.Second
	// Force qmhelper dependency for where clause generation (which doesn't
	// always happen)
	_ = qmhelper.Where
)

var attributeSchemaAfterSelectHooks []AttributeSchemaHook

var attributeSchemaBeforeInsertHooks []AttributeSchemaHook
var attributeSchemaAfterInsertHooks []AttributeSchemaHook

var attributeSchemaBeforeUpdateHooks []AttributeSchemaHook
var attributeSchemaAfterUpdateHooks []AttributeSchemaHook

var attributeSchemaBeforeDeleteHooks []AttributeSchemaHook
var attributeSchemaAfterDeleteHooks []AttributeSchemaHook

var attributeSchemaBeforeUpsertHooks []AttributeSchemaHook
var attributeSchemaAfterUpsertHooks []AttributeSchemaHook

// doAfterSelectHooks executes all "after Select" hooks.
func (o *AttributeSchema) doAfterSelectHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaAfterSelectHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeInsertHooks executes all "before insert" hooks.
func (o *AttributeSchema) doBeforeInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaBeforeInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterInsertHooks executes all "after Insert" hooks.
func (o *AttributeSchema) doAfterInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaAfterInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpdateHooks executes all "before Update" hooks.
func (o *AttributeSchema) doBeforeUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaBeforeUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpdateHooks executes all "after Update" hooks.
func (o *AttributeSchema) doAfterUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaAfterUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeDeleteHooks executes all "before Delete" hooks.
func (o *AttributeSchema) doBeforeDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaBeforeDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterDeleteHooks executes all "after Delete" hooks.
func (o *AttributeSchema) doAfterDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaAfterDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpsertHooks executes all "before Upsert" hooks.
func (o *AttributeSchema) doBeforeUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaBeforeUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpsertHooks executes all "after Upsert" hooks.
func (o *AttributeSchema) doAfterUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range attributeSchemaAfterUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// AddAttributeSchemaHook registers your hook function for all future operations.
func AddAttributeSchemaHook(hookPoint boil.HookPoint, attributeSchemaHook AttributeSchemaHook) {
	switch hookPoint {
	case boil.AfterSelectHook:
		attributeSchemaAfterSelectHooks = append(attributeSchemaAfterSelectHooks, attributeSchemaHook)
	case boil.BeforeInsertHook:
		attributeSchemaBeforeInsertHooks = append(attributeSchemaBeforeInsertHooks, attributeSchemaHook)
	case boil.AfterInsertHook:
		attributeSchemaAfterInsertHooks = append(attributeSchemaAfterInsertHooks, attributeSchemaHook)
	case boil.BeforeUpdateHook:
		attributeSchemaBeforeUpdateHooks = append(attributeSchemaBeforeUpdateHooks, attributeSchemaHook)
	case boil.AfterUpdateHook:
		attributeSchemaAfterUpdateHooks = append(attributeSchemaAfterUpdateHooks, attributeSchemaHook)
	case boil.BeforeDeleteHook:
		attributeSchemaBeforeDeleteHooks = append(attributeSchemaBeforeDeleteHooks, attributeSchemaHook)
	case boil.AfterDeleteHook:
		attributeSchemaAfterDeleteHooks = append(attributeSchemaAfterDeleteHooks, attributeSchemaHook)
	case boil.BeforeUpsertHook:
		attributeSchemaBeforeUpsertHooks = append(attributeSchemaBeforeUpsertHooks, attributeSchemaHook)
	case boil.AfterUpsertHook:
		attributeSchemaAfterUpsertHooks = append(attributeSchemaAfterUpsertHooks, attributeSchemaHook)
	}
}

// One returns a single attributeSchema record from the query.
func (q attributeSchemaQuery) One(ctx context.Context, exec boil.ContextExecutor) (*AttributeSchema, error) {
	o := &AttributeSchema{}

	queries.SetLimit(q.Query, 1)

	err := q.Bind(ctx, exec, o)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "models: failed to execute a one query for attribute_schema")
	}

	if err := o.doAfterSelectHooks(ctx, exec); err != nil {
		return o, err
	}

	return o, nil
}

// All returns all AttributeSchema records from the query.
func (q attributeSchemaQuery) All(ctx context.Context, exec boil.ContextExecutor) (AttributeSchemaSlice, error) {
	var o []*AttributeSchema

	err := q.Bind(ctx, exec, &o)
	if err != nil {
		return nil, errors.Wrap(err, "models: failed to assign all query results to AttributeSchema slice")
	}

	if len(attributeSchemaAfterSelectHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterSelectHooks(ctx, exec); err != nil {
				return o, err
			}
		}
	}

	return o, nil
}

// Count returns the count of all AttributeSchema records in the query.
func (q attributeSchemaQuery) Count(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to count attribute_schema rows")
	}

	return count, nil
}

// Exists checks if the row exists in the table.
func (q attributeSchemaQuery) Exists(ctx context.Context, exec boil.ContextExecutor) (bool, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)
	queries.SetLimit(q.Query, 1)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return false, errors.Wrap(err, "models: failed to check if attribute_schema exists")
	}

	return count > 0, nil
}

// AttributeSchemas retrieves all the records using an executor.
func AttributeSchemas(mods ...qm.QueryMod) attributeSchemaQuery {
	mods = append(mods, qm.From("\"attribute_schema\""))
	q := NewQuery(mods...)
	if len(queries.GetSelect(q)) == 0 {
		queries.SetSelect(q, []string{"\"attribute_schema\".*"})
	}

	return attributeSchemaQuery{q}
}

// FindAttributeSchema retrieves a single record by ID with an executor.
// If selectCols is empty Find will return all columns.
func FindAttributeSchema(ctx context.Context, exec boil.ContextExecutor, iD int, selectCols ...string) (*AttributeSchema, error) {
	attributeSchemaObj := &AttributeSchema{}

	sel := "*"
	if len(selectCols) > 0 {
		sel = strings.Join(strmangle.IdentQuoteSlice(dialect.LQ, dialect.RQ, selectCols), ",")
	}
	query := fmt.Sprintf(
		"select %s from \"attribute_schema\" where \"id\"=$1", sel,
	)

	q := queries.Raw(query, iD)

	err := q.Bind(ctx, exec, attributeSchemaObj)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "models: unable to select from attribute_schema")
	}

	if err = attributeSchemaObj.doAfterSelectHooks(ctx, exec); err != nil {
		return attributeSchemaObj, err
	}

	return attributeSchemaObj, nil
}

// Insert a single record using an executor.
// See boil.Columns.InsertColumnSet documentation to understand column list inference for inserts.
func (o *AttributeSchema) Insert(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) error {
	if o == nil {
		return errors.New("models: no attribute_schema provided for insertion")
	}

	var err error

	if err := o.doBeforeInsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(attributeSchemaColumnsWithDefault, o)

	key := makeCacheKey(columns, nzDefaults)
	attributeSchemaInsertCacheMut.RLock()
	cache, cached := attributeSchemaInsertCache[key]
	attributeSchemaInsertCacheMut.RUnlock()

	if !cached {
		wl, returnColumns := columns.InsertColumnSet(
			attributeSchemaAllColumns,
			attributeSchemaColumnsWithDefault,
			attributeSchemaColumnsWithoutDefault,
			nzDefaults,
		)

		cache.valueMapping, err = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, wl)
		if err != nil {
			return err
		}
		cache.retMapping, err = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, returnColumns)
		if err != nil {
			return err
		}
		if len(wl) != 0 {
			cache.query = fmt.Sprintf("INSERT INTO \"attribute_schema\" (\"%s\") %%sVALUES (%s)%%s", strings.Join(wl, "\",\""), strmangle.Placeholders(dialect.UseIndexPlaceholders, len(wl), 1, 1))
		} else {
			cache.query = "INSERT INTO \"attribute_schema\" %sDEFAULT VALUES%s"
		}

		var queryOutput, queryReturning string

		if len(cache.retMapping) != 0 {
			queryReturning = fmt.Sprintf(" RETURNING \"%s\"", strings.Join(returnColumns, "\",\""))
		}

		cache.query = fmt.Sprintf(cache.query, queryOutput, queryReturning)
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}

	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(queries.PtrsFromMapping(value, cache.retMapping)...)
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}

	if err != nil {
		return errors.Wrap(err, "models: unable to insert into attribute_schema")
	}

	if !cached {
		attributeSchemaInsertCacheMut.Lock()
		attributeSchemaInsertCache[key] = cache
		attributeSchemaInsertCacheMut.Unlock()
	}

	return o.doAfterInsertHooks(ctx, exec)
}

// Update uses an executor to update the AttributeSchema.
// See boil.Columns.UpdateColumnSet documentation to understand column list inference for updates.
// Update does not automatically update the record in case of default values. Use .Reload() to refresh the records.
func (o *AttributeSchema) Update(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) (int64, error) {
	var err error
	if err = o.doBeforeUpdateHooks(ctx, exec); err != nil {
		return 0, err
	}
	key := makeCacheKey(columns, nil)
	attributeSchemaUpdateCacheMut.RLock()
	cache, cached := attributeSchemaUpdateCache[key]
	attributeSchemaUpdateCacheMut.RUnlock()

	if !cached {
		wl := columns.UpdateColumnSet(
			attributeSchemaAllColumns,
			attributeSchemaPrimaryKeyColumns,
		)

		if !columns.IsWhitelist() {
			wl = strmangle.SetComplement(wl, []string{"created_at"})
		}
		if len(wl) == 0 {
			return 0, errors.New("models: unable to update attribute_schema, could not build whitelist")
		}

		cache.query = fmt.Sprintf("UPDATE \"attribute_schema\" SET %s WHERE %s",
			strmangle.SetParamNames("\"", "\"", 1, wl),
			strmangle.WhereClause("\"", "\"", len(wl)+1, attributeSchemaPrimaryKeyColumns),
		)
		cache.valueMapping, err = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, append(wl, attributeSchemaPrimaryKeyColumns...))
		if err != nil {
			return 0, err
		}
	}

	values := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, values)
	}
	var result sql.Result
	result, err = exec.ExecContext(ctx, cache.query, values...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update attribute_schema row")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by update for attribute_schema")
	}

	if !cached {
		attributeSchemaUpdateCacheMut.Lock()
		attributeSchemaUpdateCache[key] = cache
		attributeSchemaUpdateCacheMut.Unlock()
	}

	return rowsAff, o.doAfterUpdateHooks(ctx, exec)
}

// UpdateAll updates all rows with the specified column values.
func (q attributeSchemaQuery) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	queries.SetUpdate(q.Query, cols)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update all for attribute_schema")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to retrieve rows affected for attribute_schema")
	}

	return rowsAff, nil
}

// UpdateAll updates all rows with the specified column values, using an executor.
func (o AttributeSchemaSlice) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	ln := int64(len(o))
	if ln == 0 {
		return 0, nil
	}

	if len(cols) == 0 {
		return 0, errors.New("models: update all requires at least one column argument")
	}

	colNames := make([]string, len(cols))
	args := make([]interface{}, len(cols))

	i := 0
	for name, value := range cols {
		colNames[i] = name
		args[i] = value
		i++
	}

	// Append all of the primary key values for each column
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), attributeSchemaPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := fmt.Sprintf("UPDATE \"attribute_schema\" SET %s WHERE %s",
		strmangle.SetParamNames("\"", "\"", 1, colNames),
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), len(colNames)+1, attributeSchemaPrimaryKeyColumns, len(o)))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to update all in attributeSchema slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to retrieve rows affected all in update all attributeSchema")
	}
	return rowsAff, nil
}

// Upsert attempts an insert using an executor, and does an update or ignore on conflict.
// See boil.Columns documentation for how to properly use updateColumns and insertColumns.
func (o *AttributeSchema) Upsert(ctx context.Context, exec boil.ContextExecutor, updateOnConflict bool, conflictColumns []string, updateColumns, insertColumns boil.Columns) error {
	if o == nil {
		return errors.New("models: no attribute_schema provided for upsert")
	}

	if err := o.doBeforeUpsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(attributeSchemaColumnsWithDefault, o)

	// Build cache key in-line uglily - mysql vs psql problems
	buf := strmangle.GetBuffer()
	if updateOnConflict {
		buf.WriteByte('t')
	} else {
		buf.WriteByte('f')
	}
	buf.WriteByte('.')
	for _, c := range conflictColumns {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(updateColumns.Kind))
	for _, c := range updateColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(insertColumns.Kind))
	for _, c := range insertColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	for _, c := range nzDefaults {
		buf.WriteString(c)
	}
	key := buf.String()
	strmangle.PutBuffer(buf)

	attributeSchemaUpsertCacheMut.RLock()
	cache, cached := attributeSchemaUpsertCache[key]
	attributeSchemaUpsertCacheMut.RUnlock()

	var err error

	if !cached {
		insert, ret := insertColumns.InsertColumnSet(
			attributeSchemaAllColumns,
			attributeSchemaColumnsWithDefault,
			attributeSchemaColumnsWithoutDefault,
			nzDefaults,
		)

		update := updateColumns.UpdateColumnSet(
			attributeSchemaAllColumns,
			attributeSchemaPrimaryKeyColumns,
		)

		if updateOnConflict && len(update) == 0 {
			return errors.New("models: unable to upsert attribute_schema, could not build update column list")
		}

		conflict := conflictColumns
		if len(conflict) == 0 {
			conflict = make([]string, len(attributeSchemaPrimaryKeyColumns))
			copy(conflict, attributeSchemaPrimaryKeyColumns)
		}
		cache.query = buildUpsertQueryPostgres(dialect, "\"attribute_schema\"", updateOnConflict, ret, update, conflict, insert)

		cache.valueMapping, err = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, insert)
		if err != nil {
			return err
		}
		if len(ret) != 0 {
			cache.retMapping, err = queries.BindMapping(attributeSchemaType, attributeSchemaMapping, ret)
			if err != nil {
				return err
			}
		}
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)
	var returns []interface{}
	if len(cache.retMapping) != 0 {
		returns = queries.PtrsFromMapping(value, cache.retMapping)
	}

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}
	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(returns...)
		if errors.Is(err, sql.ErrNoRows) {
			err = nil // Postgres doesn't return anything when there's no update
		}
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}
	if err != nil {
		return errors.Wrap(err, "models: unable to upsert attribute_schema")
	}

	if !cached {
		attributeSchemaUpsertCacheMut.Lock()
		attributeSchemaUpsertCache[key] = cache
		attributeSchemaUpsertCacheMut.Unlock()
	}

	return o.doAfterUpsertHooks(ctx, exec)
}

// Delete deletes a single AttributeSchema record with an executor.
// Delete will match against the primary key column to find the record to delete.
func (o *AttributeSchema) Delete(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if o == nil {
		return 0, errors.New("models: no AttributeSchema provided for delete")
	}

	if err := o.doBeforeDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	args := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), attributeSchemaPrimaryKeyMapping)
	sql := "DELETE FROM \"attribute_schema\" WHERE \"id\"=$1"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete from attribute_schema")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by delete for attribute_schema")
	}

	if err := o.doAfterDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	return rowsAff, nil
}

// DeleteAll deletes all matching rows.
func (q attributeSchemaQuery) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if q.Query == nil {
		return 0, errors.New("models: no attributeSchemaQuery provided for delete all")
	}

	queries.SetDelete(q.Query)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete all from attribute_schema")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by deleteall for attribute_schema")
	}

	return rowsAff, nil
}

// DeleteAll deletes all rows in the slice, using an executor.
func (o AttributeSchemaSlice) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if len(o) == 0 {
		return 0, nil
	}

	if len(attributeSchemaBeforeDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doBeforeDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	var args []interface{}
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), attributeSchemaPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "DELETE FROM \"attribute_schema\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, attributeSchemaPrimaryKeyColumns, len(o))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "models: unable to delete all from attributeSchema slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "models: failed to get rows affected by deleteall for attribute_schema")
	}

	if len(attributeSchemaAfterDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	return rowsAff, nil
}

// Reload refetches the object from the database
// using the primary keys with an executor.
func (o *AttributeSchema) Reload(ctx context.Context, exec boil.ContextExecutor) error {
	ret, err := FindAttributeSchema(ctx, exec, o.ID)
	if err != nil {
		return err
	}

	*o = *ret
	return nil
}

// ReloadAll refetches every row with matching primary key column values
// and overwrites the original object slice with the newly updated slice.
func (o *AttributeSchemaSlice) ReloadAll(ctx context.Context, exec boil.ContextExecutor) error {
	if o == nil || len(*o) == 0 {
		return nil
	}

	slice := AttributeSchemaSlice{}
	var args []interface{}
	for _, obj := range *o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), attributeSchemaPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "SELECT \"attribute_schema\".* FROM \"attribute_schema\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, attributeSchemaPrimaryKeyColumns, len(*o))

	q := queries.Raw(sql, args...)

	err := q.Bind(ctx, exec, &slice)
	if err != nil {
		return errors.Wrap(err, "models: unable to reload all in AttributeSchemaSlice")
	}

	*o = slice

	return nil
}

// AttributeSchemaExists checks if the AttributeSchema row exists.
func AttributeSchemaExists(ctx context.Context, exec boil.ContextExecutor, iD int) (bool, error) {
	var exists bool
	sql := "select exists(select 1 from \"attribute_schema\" where \"id\"=$1 limit 1)"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, iD)
	}
	row := exec.QueryRowContext(ctx, sql, iD)

	err := row.Scan(&exists)
	if err != nil {
		return false, errors.Wrap(err, "models: unable to check if attribute_schema exists")
	}

	return exists, nil
}
